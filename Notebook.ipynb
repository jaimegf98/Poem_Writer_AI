{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of what we are going to create. Some lines generated by the program.\n",
    "\n",
    "\n",
    "\n",
    "**I  every be that pull in i every i life go would be time the soul a how empty has i spread but be day of and it me will i life go would be has of and it me will i life go would be has of and it me will i life go would be has of and it me will i life go would be has of and it me will i life go would be has of and it me will i life go would be has of and it me will i life go would be.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries\n",
    "We import the necessary libraries useful for tokenizing and padding of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing the Data\n",
    "Here, we will be tokenizing the lines of the poem and convert them to sequences of equal lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Extracting the raw text from the file\n",
    "We read the text available in the file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would it be ok if I took some of your time?\n",
      "Would it be ok if I wrote you a rhyme?\n",
      "Would it be ok if I opened my heart?\n",
      "Would it be ok if I took on the part\n",
      "Of being your man and showed you a view,\n",
      "One that only a real man could do?\n",
      "Would it be ok if I could make you smile?\n",
      "Would it be ok if I held you awhile?\n",
      "Would it be ok if I kissed your face?\n",
      "Would it be ok if I were to replace\n",
      "All the men in your past that just wouldn't do\n",
      "And vow to be faithful and always be true?\n",
      "Would it be alright to look in your eyes?\n",
      "Would it be alright to never tell lies?\n",
      "Would it be alright to find a way?\n",
      "Would it be alright to long for the day\n",
      "To pull you close and whisper in your ear\n",
      "And tell you our feelings are nothing to fear?\n",
      "Would it be ok if I took some of your time?\n",
      "Would it be ok if I wrote you a rhyme?\n",
      "To tell you there's nothing I'd rather do\n",
      "Than spend my whole life loving only you...\n",
      "\n",
      "i carry your heart with me(i carry it in\n",
      "my heart)i am never without it(anywhere\n",
      "i go you go,my dear; and whatever is done\n",
      "by only me is your doing,my darling)\n",
      "i fear\n",
      "no fate(for you are my fate,my sweet)i want\n",
      "no world(for beautiful you are my world,my true)\n",
      "and it's you are whatever a moon has always meant\n",
      "and whatever a sun will always sing is you\n",
      "here is the deepest secret nobody knows\n",
      "(here is the root of the root and the bud of the bud\n",
      "and the sky of the sky of a tree called life;which grows\n",
      "higher than the soul can hope or mind can hide)\n",
      "and this is the wonder that's keeping the stars apart\n",
      "\n",
      "Love can't be described.\n",
      "It has no shape, it has no form.\n",
      "Love is not an object,\n",
      "Love does not conform.\n",
      "Love enters our lives\n",
      "The moment we are born.\n",
      "From the cradle to the grave,\n",
      "Love's in everyone.\n",
      "Love burns like a candle\n",
      "That sometimes flickers but never dies.\n",
      "Love may be invisible,\n",
      "Although it's right before your eyes,\n",
      "\n",
      "Love is universal,\n",
      "It encompasses the globe.\n",
      "No matter where you are,\n",
      "Love has a language all its own.\n",
      "Love is all around you,\n",
      "There's plenty of love to spare.\n",
      "You cannot see or touch it,\n",
      "But love is everywhere.\n",
      "Love's the greatest power,\n",
      "And yet it is so small.\n",
      "Love's a gift from God,\n",
      "To be shared amongst us all.\n",
      "\n",
      "Don't go far off, not even for a day, because --\n",
      "because -- I don't know how to say it: a day is long\n",
      "and I will be waiting for you, as in an empty station\n",
      "when the trains are parked off somewhere else, asleep.\n",
      "Don't leave me, even for an hour, because\n",
      "then the little drops of anguish will all run together,\n",
      "the smoke that roams looking for a home will drift\n",
      "into me, choking my lost heart.\n",
      "Oh, may your silhouette never dissolve on the beach;\n",
      "may your eyelids never flutter into the empty distance.\n",
      "Don't leave me for a second, my dearest,\n",
      "because in that moment you'll have gone so far\n",
      "I'll wander mazily over all the earth, asking,\n",
      "Will you come back? Will you leave me here, dying?\n",
      "\n",
      "Your lips so soft and red.\n",
      "The thought of kissing you is stuck in my head.\n",
      "Your beauty so bright and warm,\n",
      "shinning through the darkest storm.\n",
      "\n",
      "Some people believe there is a one,\n",
      "The one who points your way to the sun.\n",
      "A person they believe makes them complete,\n",
      "The one who will support when facing defeat.\n",
      "\n",
      "Ever since\n",
      "I laid eyes on you,\n",
      "I knew for a fact\n",
      "This was a dream come true.\n",
      "\n",
      "Baby, I love you so.\n",
      "Baby, you are my heart and soul.\n",
      "I feel I could spread wings and fly\n",
      "Every time I gaze into your eyes.\n",
      "\n",
      "As I lie in my bed,\n",
      "Your name's running through my head.\n",
      "All I can think of is you,\n",
      "All that you do,\n",
      "\n",
      "Thoughts of you surround me.\n",
      "You're the beating of my heart.\n",
      "The love you give defines me.\n",
      "My life is no longer dark.\n",
      "\n",
      "The moment I met you,\n",
      "I knew I was in love,\n",
      "Like an angel had sent you from up above.\n",
      "The connection we share grows stronger every day.\n",
      "\n",
      "Remember when we first did meet,\n",
      "our hearts rejoiced and skipped a beat.\n",
      "Remember our first kiss goodnight,\n",
      "the hug we shared that summer's night.\n",
      "\n",
      "She pulled up the drive, and as she got out,\n",
      "I said to myself, \"There is no doubt,\n",
      "The most beautiful thing I've seen in my life.\n",
      "This woman before me will be my wife.\"\n",
      "\n",
      "Let me take care of your broken heart\n",
      "and show you how to fly.\n",
      "Let me hold you gently by the hand\n",
      "and kiss your tears goodbye.\n",
      "\n",
      "A special place for you and me\n",
      "An undying bond to guide us free\n",
      "Loneliness blocking the day\n",
      "Our Love lighting the way\n",
      "\n",
      "I Do Not Love You Except Because I Love You - Poem by Pablo Neruda\n",
      "Autoplay next video I do not love you except because I love you; \n",
      "I go from loving to not loving you,\n",
      "From waiting to not waiting for you\n",
      "My heart moves from cold to fire.\n",
      "\n",
      "I love you only because it's you the one I love; \n",
      "I hate you deeply, and hating you\n",
      "Bend to you, and the measure of my changing love for you\n",
      "Is that I do not see you but love you blindly.\n",
      "\n",
      "Maybe January light will consume\n",
      "My heart with its cruel\n",
      "Ray, stealing my key to true calm.\n",
      "\n",
      "In this part of the story I am the one who\n",
      "Dies, the only one, and I will die of love because I love you,\n",
      "Because I love you, Love, in fire and blood. \n",
      "\n",
      "My love for you is like the raging sea,\n",
      "So powerful and deep it will forever be.\n",
      "Through storm, wind, and heavy rain,\n",
      "It will withstand every pain.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_file_path = 'text'\n",
    "\n",
    "def get_raw_data_from_file( path ):\n",
    "    text = str()\n",
    "    with open(path, \"r\") as fd:\n",
    "        text += fd.read()\n",
    "    return text\n",
    "\n",
    "raw_text = get_raw_data_from_file( text_file_path )\n",
    "print( raw_text )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the lines of the poem\n",
    "We tokenize poem lines using the tensorflow.keras.preprocessing.text.Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "corpus = raw_text.split( \"\\n\\n\" )\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len( tokenizer.word_index ) + 1\n",
    "\n",
    "input_sequences = []\n",
    "\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the sequences of tokenized lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 14] (1019, 202)\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.] (1019, 392)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sequence_lengths = list()\n",
    "for x in input_sequences:\n",
    "    sequence_lengths.append( len( x ) )\n",
    "max_sequence_len = max( sequence_lengths )\n",
    "\n",
    "input_sequences = np.array(pad_sequences(input_sequences,\n",
    "                                         maxlen=max_sequence_len+1, padding='pre'))\n",
    "x, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "print( x[ 0 ] , x.shape )\n",
    "print( y[ 0 ]  , y.shape ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing and Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow as tf\n",
    "\n",
    "dropout_rate = 0.3\n",
    "activation_func = keras.activations.relu\n",
    "\n",
    "SCHEMA = [\n",
    "\n",
    "    Embedding( total_words , 10, input_length=max_sequence_len ),\n",
    "    LSTM( 32 ) ,\n",
    "    Dropout(dropout_rate),\n",
    "    Dense( 32 , activation=activation_func ) ,\n",
    "    Dropout(dropout_rate),\n",
    "    Dense( total_words, activation=tf.nn.softmax )\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 202, 10)           3920      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                5504      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 392)               12936     \n",
      "=================================================================\n",
      "Total params: 23,416\n",
      "Trainable params: 23,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = keras.Sequential(SCHEMA)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam() ,\n",
    "    loss=keras.losses.categorical_crossentropy ,\n",
    "    metrics=[ 'accuracy' ]\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1019/1019 [==============================] - 3s 3ms/step - loss: 5.8119 - acc: 0.0285\n",
      "Wall time: 2.87 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b4b07f9ac8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    batch_size=50 ,\n",
    "    epochs=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save( 'model.h5' ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter some starter text ( I want ... ) :  Hello\n",
      "Enter the desired length of the generated sentence :  90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict(seed_text , seed=10 ):\n",
    "\n",
    "    for i in range( seed ):\n",
    "\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=\n",
    "        max_sequence_len , padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0 )\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    return seed_text\n",
    "\n",
    "print( predict( input( 'Enter some starter text ( I want ... ) : ') , int( input( 'Enter the desired length of the generated sentence : '))  ) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
